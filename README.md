# Flight-Booking-Data-Ingestion-CICD
# ğŸ“Œ Flight Booking Data Ingestion CICD  

## ğŸ“– Overview  
This project demonstrates a **data ingestion and transformation pipeline** using:  
- **GitHub Actions** for CI/CD  
- **Apache Airflow (Cloud Composer on GCP)** for orchestration  
- **Apache Spark (Dataproc Serverless)** for data processing  
- **Google Cloud Storage (GCS)** for raw and intermediate data  
- **Google BigQuery** for storage and analytics  

The pipeline automates deployment, monitors new data arrivals, processes flight booking data, and generates insights for analysis.  

---

## ğŸ–¼ï¸ Project Diagram  

![Flight Booking Data Ingestion CICD](./Flight-Booking.png)  

---

## ğŸš€ Architecture  

### Workflow  
1. **GitHub Workflow (CI/CD)**  
   - Push to `dev` branch â†’ Deploys to **Airflow_Dev**  
   - Push to `main` branch â†’ Deploys to **Airflow_Prod**  
   - Uploads Airflow DAGs, variables, and Spark jobs to GCP  

2. **Airflow DAG (Orchestration using Cloud Composer)**  
   - Runs on **Google Cloud Composer**, a managed Airflow service.  
   - Senses the presence of the `flight.csv` file in Cloud Storage.  
   - Submits a Dataproc Serverless Spark job once the file is available.  

3. **Dataproc Serverless (Processing Layer)**  
   - Runs a PySpark job without needing cluster management.  
   - Reads the raw flight data from Cloud Storage.  
   - Applies transformations and generates insights.  
   - Writes output directly into BigQuery.  

4. **Google Cloud Storage (Data Lake)**  
   - Stores raw input data (`flight.csv`).  
   - Stores deployed artifacts such as Spark job scripts and Airflow variable files.  

5. **Google BigQuery (Data Warehouse)**  
   - Stores the processed and transformed data.  
   - Provides analytical tables for downstream BI and reporting.  
   - Separate tables for **dev** and **prod** environments to ensure safe testing.  

---

## ğŸ—‚ Project Structure  

```bash
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ flight_booking_cicd.yml      # GitHub Actions workflow
â”œâ”€â”€ airflow_job/
â”‚   â””â”€â”€ airflow_job.py               # Airflow DAG (Dataproc trigger)
â”œâ”€â”€ spark_job/
â”‚   â””â”€â”€ spark_transformation_job.py  # PySpark job for transformations
â”œâ”€â”€ variables/
â”‚   â”œâ”€â”€ dev/variables.json           # Airflow variables for DEV
â”‚   â””â”€â”€ prod/variables.json          # Airflow variables for PROD
â””â”€â”€ README.md                        # Project documentation
